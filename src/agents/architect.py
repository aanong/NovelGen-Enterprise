import os
import re
import json
from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from ..schemas.state import PlotPoint, NGEState, ForeshadowingSchema, ForeshadowingStatus
from ..config import Config
from ..utils import strip_think_tags, extract_json_from_text, normalize_llm_content
from ..db.vector_store import VectorStore
from .base import BaseAgent
from dotenv import load_dotenv

load_dotenv()


class ForeshadowingStrategy(BaseModel):
    """ä¼ç¬”å¤„ç†ç­–ç•¥"""
    advance: List[str] = Field(default_factory=list, description="æœ¬ç« åº”æ¨è¿›çš„ä¼ç¬”")
    resolve: List[str] = Field(default_factory=list, description="æœ¬ç« åº”å›æ”¶çš„ä¼ç¬”")
    plant: List[str] = Field(default_factory=list, description="æœ¬ç« åº”åŸ‹è®¾çš„æ–°ä¼ç¬”")
    reasoning: str = Field(default="", description="ç­–ç•¥ç†ç”±")

class OutlineExpansion(BaseModel):
    expanded_points: List[PlotPoint] = Field(description="è¯¦ç»†çš„å¤§çº²åˆ—è¡¨ï¼Œç²¾ç¡®åˆ°åœºé¢è°ƒåº¦")

class ChapterPlan(BaseModel):
    thinking: str = Field(description="æ€è€ƒè¿‡ç¨‹ï¼šæœ¬ç« å¦‚ä½•æœåŠ¡ä¸»çº¿ï¼Ÿå¦‚ä½•æ‰¿æ¥ä¸Šæ–‡ï¼Ÿ")
    scene_description: str = Field(description="æ ¸å¿ƒåœºé¢è°ƒåº¦æè¿°")
    key_conflict: str = Field(description="æ ¸å¿ƒå†²çªç‚¹ä¸é«˜æ½®")
    instruction: str = Field(description="ç»™ Writer Agent çš„å…·ä½“å†™ä½œæŒ‡ä»¤ï¼ŒåŒ…å«è¯­æ°”ã€è§†è§’è¦æ±‚")

class ChapterOutline(BaseModel):
    chapter_number: int = Field(description="ç« èŠ‚åºå·")
    title: str = Field(description="ç« èŠ‚æ ‡é¢˜")
    scene_description: str = Field(description="æ ¸å¿ƒåœºé¢è°ƒåº¦æè¿°")
    key_conflict: str = Field(description="æ ¸å¿ƒå†²çªç‚¹ä¸é«˜æ½®")
    foreshadowing: List[str] = Field(default_factory=list, description="æœ¬ç« åŸ‹ä¸‹çš„ä¼ç¬”")

class FullNovelOutline(BaseModel):
    chapters: List[ChapterOutline] = Field(description="å…¨ä¹¦åˆ†ç« å¤§çº²åˆ—è¡¨")

class ArchitectAgent(BaseAgent):
    """
    Architect Agent (Gemini): è´Ÿè´£å‰§æƒ…é€»è¾‘ã€å¤§çº²æ„å»ºä¸æ‹†è§£ã€‚
    åˆ©ç”¨ Gemini çš„é€»è¾‘æ¨æ¼”èƒ½åŠ›ï¼Œç¡®ä¿ä¸–ç•Œè§‚ä¸€è‡´æ€§ã€‚
    éµå¾ª Rule 1.1: Gemini ä¸ºç‹ï¼ˆåº•å±‚é€»è¾‘æœ€ç»ˆè§£é‡Šæƒï¼‰
    """
    def __init__(self, temperature: Optional[float] = None):
        super().__init__(
            model_name="gemini",
            temperature=temperature or Config.model.GEMINI_TEMPERATURE,
            mock_responses=[
                # Response for generate_chapter_outlines
                json.dumps({"chapters": [
                    {"chapter_number": 1, "title": "Ch1", "scene_description": "Scene 1", "key_conflict": "Conflict 1", "foreshadowing": []},
                    {"chapter_number": 2, "title": "Ch2", "scene_description": "Scene 2", "key_conflict": "Conflict 2", "foreshadowing": []},
                    {"chapter_number": 3, "title": "Ch3", "scene_description": "Scene 3", "key_conflict": "Conflict 3", "foreshadowing": []}
                ]}),
                # Response for plan_next_chapter (called by Writer/Graph)
                json.dumps({"thinking": "Think", "scene_description": "Scene", "key_conflict": "Conflict", "instruction": "Write"}),
                # More responses if needed
                json.dumps({"thinking": "Think", "scene_description": "Scene", "key_conflict": "Conflict", "instruction": "Write"}),
            ]
        )
        self.outline_parser = PydanticOutputParser(pydantic_object=OutlineExpansion)
        self.plan_parser = PydanticOutputParser(pydantic_object=ChapterPlan)
        self.full_outline_parser = PydanticOutputParser(pydantic_object=FullNovelOutline)
        self.vector_store = VectorStore()

    async def process(self, state: NGEState, **kwargs) -> Dict[str, Any]:
        """
        BaseAgent required method.
        Delegates to plan_next_chapter for the main workflow.
        """
        return await self.plan_next_chapter(state)

    async def generate_chapter_outlines(self, synopsis: str, world_view: str, total_chapters: int = 10) -> List[ChapterOutline]:
        """
        Rule 3.4: å…¨ä¹¦å¤§çº²é¢„ç”Ÿæˆ
        å°†æ ¸å¿ƒæ¢—æ¦‚æ‹†è§£ä¸ºå…·ä½“çš„åˆ†ç« å¤§çº²ã€‚
        """
        # æ£€ç´¢å‚è€ƒèµ„æ–™
        references = await self.vector_store.search_references(synopsis, top_k=2)
        ref_context = ""
        if references:
            ref_context = "\nã€å‚è€ƒèµ„æ–™ã€‘\n"
            for ref in references:
                ref_context += f"- {ref['title']}: {ref['content'][:100]}...\n"

        prompt = ChatPromptTemplate.from_messages([
            ("system", (
                "ä½ æ˜¯ä¸€ä¸ªç½‘æ–‡æ€»ç­–åˆ’ã€‚ä»»åŠ¡æ˜¯å°†ä¸€ä¸ªç®€çŸ­çš„æ•…äº‹æ¢—æ¦‚æ‹†è§£ä¸ºè¯¦ç»†çš„åˆ†ç« å¤§çº²ã€‚\n"
                "ã€è¦æ±‚ã€‘\n"
                "1. æ€»å…±ç”Ÿæˆçº¦ {total_chapters} ç« ã€‚\n"
                "2. æ¯ä¸€ç« éƒ½è¦æœ‰æ˜ç¡®çš„å†²çªå’Œæ¨è¿›ã€‚\n"
                "3. ä¸¥æ ¼éµå®ˆä¸–ç•Œè§‚è®¾å®šï¼š{world_view}\n"
                "4. ç¡®ä¿å‰§æƒ…èŠ‚å¥å¼ å¼›æœ‰åº¦ï¼ˆèµ·æ‰¿è½¬åˆï¼‰ã€‚\n"
                "{ref_context}\n"
                "è¾“å‡ºæ ¼å¼å¿…é¡»ä¸º JSONã€‚\n"
                "{format_instructions}"
            )),
            ("human", "æ•…äº‹æ¢—æ¦‚ï¼š\n{synopsis}")
        ])

        chain = prompt | self.llm | self.full_outline_parser
        try:
            result = await chain.ainvoke({
                "world_view": world_view,
                "synopsis": synopsis,
                "total_chapters": total_chapters,
                "ref_context": ref_context,
                "format_instructions": self.full_outline_parser.get_format_instructions()
            })
            return result.chapters
        except Exception as e:
            print(f"Outline Generation Error: {e}")
            return []

    async def refine_outline(self, current_outlines: List[Dict[str, Any]], instruction: str, start_chapter: int, world_view: str) -> List[ChapterOutline]:
        """
        è°ƒæ•´ç°æœ‰å¤§çº²ï¼šä» start_chapter å¼€å§‹ï¼Œæ ¹æ® instruction é‡æ–°è§„åˆ’åç»­ç« èŠ‚ã€‚
        """
        # æå–å‰æ–‡æ‘˜è¦ï¼ˆstart_chapter ä¹‹å‰çš„å†…å®¹ï¼‰
        context_summary = "\n".join([
            f"ç¬¬ {o['chapter_number']} ç« : {o['scene_description']}" 
            for o in current_outlines if o['chapter_number'] < start_chapter
        ])
        
        remaining_chapters = len(current_outlines) - (start_chapter - 1)
        if remaining_chapters < 1:
            remaining_chapters = 5 # é»˜è®¤ç»­å†™ 5 ç« 
            
        prompt = ChatPromptTemplate.from_messages([
            ("system", (
                "ä½ æ˜¯ä¸€ä¸ªç½‘æ–‡ä¸»ç¼–ã€‚ä»»åŠ¡æ˜¯æ ¹æ®ä¿®æ”¹æ„è§ï¼Œé‡æ–°è§„åˆ’å°è¯´åç»­çš„å¤§çº²ã€‚\n"
                "ã€å‰æƒ…æè¦ã€‘\n{context_summary}\n\n"
                "ã€ä¿®æ”¹è¦æ±‚ã€‘\n{instruction}\n\n"
                "ã€ä¸–ç•Œè§‚ã€‘\n{world_view}\n\n"
                "è¯·ä»ç¬¬ {start_chapter} ç« å¼€å§‹ï¼Œé‡æ–°ç”Ÿæˆçº¦ {remaining_chapters} ç« çš„å¤§çº²ã€‚\n"
                "è¾“å‡ºæ ¼å¼å¿…é¡»ä¸º JSONã€‚\n"
                "{format_instructions}"
            )),
            ("human", "è¯·å¼€å§‹é‡æ–°è§„åˆ’ã€‚")
        ])
        
        chain = prompt | self.llm | self.full_outline_parser
        try:
            result = await chain.ainvoke({
                "context_summary": context_summary,
                "instruction": instruction,
                "world_view": world_view,
                "start_chapter": start_chapter,
                "remaining_chapters": remaining_chapters,
                "format_instructions": self.full_outline_parser.get_format_instructions()
            })
            # ä¿®æ­£ç« èŠ‚å·ï¼Œç¡®ä¿è¿ç»­
            for i, ch in enumerate(result.chapters):
                ch.chapter_number = start_chapter + i
            return result.chapters
        except Exception as e:
            print(f"Outline Refinement Error: {e}")
            return []

    async def expand_outline(self, user_prompt: str, world_view: str) -> List[PlotPoint]:
        """
        æ ¹æ®ç”¨æˆ·è¾“å…¥çš„ç®€å•å¤§çº²ï¼Œæ‰©å……ä¸ºç²¾ç»†åŒ–çš„å¤§çº²ã€‚
        """
        # æ£€ç´¢å‚è€ƒèµ„æ–™
        references = await self.vector_store.search_references(user_prompt, top_k=2)
        ref_context = ""
        if references:
            ref_context = "\nã€å‚è€ƒèµ„æ–™ã€‘\n"
            for ref in references:
                ref_context += f"- {ref['title']}: {ref['content'][:100]}...\n"

        prompt = ChatPromptTemplate.from_messages([
            ("system", (
                "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç½‘æ–‡ä¸»ç¼–å’Œæ¶æ„å¸ˆã€‚æ“…é•¿æ„å»ºé€»è¾‘ä¸¥å¯†ã€èŠ‚å¥æ„Ÿå¼ºçš„å°è¯´å¤§çº²ã€‚\n"
                "å¿…é¡»éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n"
                "1. ä¸¥ç¦é€»è¾‘æ¼æ´ã€‚\n"
                "2. æ¯ä¸ªå‰§æƒ…ç‚¹å¿…é¡»åŒ…å«æ˜ç¡®çš„å†²çªå’Œæ¨è¿›ä½œç”¨ã€‚\n"
                "3. ä¸–ç•Œè§‚é™åˆ¶ï¼š{world_view}\n"
                "{ref_context}\n"
                "è¾“å‡ºæ ¼å¼å¿…é¡»ä¸º JSONã€‚\n"
                "{format_instructions}"
            )),
            ("human", "è¯·æ ¹æ®ä»¥ä¸‹ç®€è¿°æ‰©å……å¤§çº²ï¼š\n{user_prompt}")
        ])

        chain = prompt | self.llm | self.outline_parser
        result = await chain.ainvoke({
            "world_view": world_view,
            "user_prompt": user_prompt,
            "ref_context": ref_context,
            "format_instructions": self.outline_parser.get_format_instructions()
        })
        return result.expanded_points

    async def check_coherence(self, state: NGEState, plan_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ£€æŸ¥è§„åˆ’ä¸å‰æ–‡çš„è¿è´¯æ€§ã€‚
        """
        recent_summaries = getattr(getattr(state, "memory_context", None), "recent_summaries", [])
        last_summary = "\n".join([f"- {s}" for s in recent_summaries[-3:]]) if recent_summaries else "æ— å‰æ–‡"
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", (
                "ä½ æ˜¯ä¸€ä¸ªæå…¶ä¸¥è°¨çš„å°è¯´é€»è¾‘å®¡æŸ¥å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ£€æŸ¥ã€æ‹Ÿå®šçš„ä¸‹ä¸€ç« è§„åˆ’ã€‘ä¸ã€å‰æ–‡å†…å®¹ã€‘æ˜¯å¦å­˜åœ¨é€»è¾‘çŸ›ç›¾ã€‚\n"
                "é‡ç‚¹æ£€æŸ¥ï¼š\n"
                "1. æ—¶é—´çº¿æ˜¯å¦å†²çªï¼Ÿ\n"
                "2. è§’è‰²ä½ç½®ã€çŠ¶æ€æ˜¯å¦è¿ç»­ï¼Ÿ\n"
                "3. æ ¸å¿ƒè®¾å®šæ˜¯å¦è¢«è¿åï¼Ÿ\n"
                "è¾“å‡ºæ ¼å¼ï¼šJSONï¼ŒåŒ…å« coherent (bool) å’Œ issues (list of str)ã€‚"
            )),
            ("human", (
                "ã€å‰æ–‡æ‘˜è¦å›é¡¾ã€‘\n{last_summary}\n\n"
                "ã€æ‹Ÿå®šçš„è§„åˆ’ã€‘\n"
                "åœºæ™¯ï¼š{scene}\n"
                "å†²çªï¼š{conflict}\n"
                "æŒ‡ä»¤ï¼š{instruction}\n\n"
                "è¯·è¯„ä¼°å…¶è¿è´¯æ€§ã€‚"
            ))
        ])
        
        try:
            messages = prompt.format_messages(
                last_summary=last_summary,
                scene=plan_data.get("scene", ""),
                conflict=plan_data.get("conflict", ""),
                instruction=plan_data.get("instruction", "")
            )
            response = await self.llm.ainvoke(messages)
            content = strip_think_tags(normalize_llm_content(response.content))
            return extract_json_from_text(content)
        except Exception as e:
            print(f"Coherence Check Error: {e}")
            return {"coherent": True, "issues": []}

    async def plan_next_chapter(self, state: NGEState) -> Dict[str, Any]:
        """
        å†™æ¯ä¸€ç« å‰ï¼ŒAgent å¿…é¡»å›ç­”ï¼š"è¿™ä¸€ç« å¦‚ä½•æœåŠ¡äºä¸»çº¿ï¼Ÿä¸Šä¸€ç« çš„ç»“å°¾æ˜¯ä»€ä¹ˆï¼Ÿ"
        éµå¾ª Rule 1.1: Gemini ä¸ºç‹ï¼Œä¸å¾—è‡ªè¡Œä¿®æ”¹è®¾å®š
        
        å¢å¼ºåŠŸèƒ½ï¼š
        1. æ£€æŸ¥å³å°†åˆ°æœŸçš„ä¼ç¬”
        2. å¼ºåˆ¶åœ¨è§„åˆ’ä¸­åŒ…å«è¿‡æœŸä¼ç¬”çš„å¤„ç†æŒ‡ä»¤
        3. ç”Ÿæˆä¼ç¬”æ¨è¿›ç­–ç•¥
        """
        current_chapter = state.current_plot_index + 1
        
        if not state.plot_progress or state.current_plot_index >= len(state.plot_progress):
            current_point_info = "å‰§æƒ…è‡ªç”±å‘å±•é˜¶æ®µ"
        else:
            current_point = state.plot_progress[state.current_plot_index]
            key_events = getattr(current_point, "key_events", []) or []
            current_point_info = (
                f"æ ‡é¢˜ï¼š{getattr(current_point, 'title', '')}\n"
                f"æè¿°ï¼š{getattr(current_point, 'description', '')}\n"
                f"å…³é”®äº‹ä»¶ï¼š{', '.join(key_events)}"
            )

        recent_summaries = getattr(getattr(state, "memory_context", None), "recent_summaries", None) or []
        if recent_summaries:
            last_summary = "\n".join([f"- {s}" for s in recent_summaries])
        else:
            last_summary = "å¼€ç¯‡ç¬¬ä¸€ç« "

        characters = getattr(state, "characters", None) or {}
        if characters:
            char_info = "\n".join([
                (
                    f"- {name}: "
                    f"æŠ€èƒ½={', '.join(getattr(char, 'skills', []) or [])}, "
                    f"èµ„äº§={json.dumps(getattr(char, 'assets', {}) or {}, ensure_ascii=False)}, "
                    f"ç‰©å“={', '.join([getattr(i, 'name', str(i)) for i in (getattr(char, 'inventory', []) or [])])}"
                )
                for name, char in characters.items()
            ])
        else:
            char_info = "æ— "

        # ========== å¢å¼ºï¼šä¼ç¬”å›æ”¶æ—¶æœºè§„åˆ’ ==========
        # è·å–ç»“æ„åŒ–ä¼ç¬”ï¼ˆä¼˜å…ˆï¼‰å’Œä¼ ç»Ÿä¼ç¬”åˆ—è¡¨
        foreshadowing_analysis = self._analyze_foreshadowing_timing(state, current_chapter)
        threads_str = foreshadowing_analysis["threads_str"]
        foreshadowing_urgency = foreshadowing_analysis["urgency_prompt"]
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", (
                "ä½ æ˜¯ä¸€ä¸ªå‰§æƒ…è§„åˆ’ä¸“å®¶ã€‚ä»»åŠ¡æ˜¯ä¸ºå³å°†æ’°å†™çš„ç« èŠ‚åˆ¶å®šè¯¦ç»†çš„å¾®å‹æçº²ã€‚\n"
                "å¿…é¡»è¾“å‡º JSON æ ¼å¼ï¼ŒåŒ…å« thinking, scene_description, key_conflict, instruction, foreshadowing_strategyã€‚\n\n"
                "ã€ä¼ç¬”ç®¡ç†è¦æ±‚ã€‘ï¼š\n"
                "1. æ¨è¿›ç­–ç•¥ï¼šæ˜ç¡®æŒ‡å‡ºæœ¬ç« åº”æ¨è¿›å“ªäº›å·²æœ‰çš„å­˜é‡ä¼ç¬”ã€‚\n"
                "2. å›æ”¶ç­–ç•¥ï¼šå¦‚æœå‰§æƒ…æ—¶æœºæˆç†Ÿï¼Œæ˜ç¡®æŒ‡å‡ºåº”åœ¨æœ¬ç« å›æ”¶/æ­æ™“çš„ä¼ç¬”ã€‚\n"
                "3. åŸ‹çº¿ç­–ç•¥ï¼šæ ¹æ®ä¸»çº¿éœ€è¦ï¼Œæœ¬ç« æ˜¯å¦éœ€è¦åŸ‹ä¸‹æ–°çš„ä¼ç¬”æˆ–æ‚¬å¿µã€‚\n\n"
                "ã€ä¼ç¬”ç´§æ€¥æ€§è¯´æ˜ã€‘ï¼š\n"
                "{foreshadowing_urgency}\n\n"
                "ã€foreshadowing_strategy æ ¼å¼ã€‘ï¼š\n"
                "{{\n"
                '  "advance": ["ä¼ç¬”Açš„æ¨è¿›æ–¹å¼", "ä¼ç¬”Bçš„æ¨è¿›æ–¹å¼"],\n'
                '  "resolve": ["åº”å›æ”¶çš„ä¼ç¬”åŠå›æ”¶æ–¹å¼"],\n'
                '  "plant": ["æ–°åŸ‹è®¾çš„ä¼ç¬”æè¿°"],\n'
                '  "reasoning": "ç­–ç•¥ç†ç”±"\n'
                "}}\n\n"
                "{format_instructions}"
            )),
            ("human", (
                "ã€å‰æ–‡æ‘˜è¦ã€‘\n{last_summary}\n\n"
                "ã€äººç‰©ä¿¡æ¯ã€‘\n{char_info}\n\n"
                "ã€æœªå›æ”¶ä¼ç¬”/æ‚¬å¿µã€‘\n{threads_str}\n\n"
                "ã€å½“å‰å‰§æƒ…ç‚¹ã€‘\n{current_point_info}\n\n"
                "è¯·è§„åˆ’ä¸‹ä¸€ç« è¯¦æƒ…ã€‚åœ¨ instruction ä¸­æ˜¾å¼åŒ…å«å¯¹ä¼ç¬”å¤„ç†çš„å…·ä½“å†™ä½œæŒ‡ä»¤ã€‚"
            ))
        ])

        messages = prompt.format_messages(
            last_summary=last_summary,
            char_info=char_info,
            threads_str=threads_str,
            current_point_info=current_point_info,
            foreshadowing_urgency=foreshadowing_urgency,
            format_instructions=self.plan_parser.get_format_instructions()
        )

        try:
            response = await self.llm.ainvoke(messages)
            content_str = strip_think_tags(normalize_llm_content(response.content))
            plan_json = extract_json_from_text(content_str)

            if isinstance(plan_json, dict) and plan_json:
                # æå–ä¼ç¬”ç­–ç•¥
                foreshadowing_strategy = plan_json.get("foreshadowing_strategy", {})
                
                # æ„å»ºå¢å¼ºçš„æŒ‡ä»¤ï¼ŒåŒ…å«ä¼ç¬”å¤„ç†è¦æ±‚
                instruction = plan_json.get("instruction") or f"è¯·ç»§ç»­å†™ä¸‹ä¸€ç« ã€‚åŸºäºå‰§æƒ…ç‚¹ï¼š{current_point_info}"
                
                # å¦‚æœæœ‰è¿‡æœŸä¼ç¬”ï¼Œå¼ºåˆ¶æ·»åŠ å›æ”¶æŒ‡ä»¤
                if foreshadowing_analysis["overdue"]:
                    overdue_instruction = self._build_overdue_instruction(foreshadowing_analysis["overdue"])
                    instruction = f"{instruction}\n\n{overdue_instruction}"
                
                return {
                    "scene": plan_json.get("scene_description") or plan_json.get("scene") or "æœªçŸ¥åœºæ™¯",
                    "conflict": plan_json.get("key_conflict") or plan_json.get("conflict") or "æœªçŸ¥å†²çª",
                    "instruction": instruction,
                    "foreshadowing_strategy": foreshadowing_strategy,
                }

            raise ValueError(f"Could not find JSON in response: {content_str}")

        except Exception as e:
            print(f"Plan Error: {e}")
            return {
                "scene": "æœªçŸ¥åœºæ™¯",
                "conflict": "æœªçŸ¥å†²çª",
                "instruction": f"è¯·ç»§ç»­å†™ä¸‹ä¸€ç« ã€‚åŸºäºå‰§æƒ…ç‚¹ï¼š{current_point_info}",
                "foreshadowing_strategy": {},
            }
    
    def _analyze_foreshadowing_timing(
        self, 
        state: NGEState, 
        current_chapter: int
    ) -> Dict[str, Any]:
        """
        åˆ†æä¼ç¬”å›æ”¶æ—¶æœº
        
        Args:
            state: å½“å‰çŠ¶æ€
            current_chapter: å½“å‰ç« èŠ‚å·
            
        Returns:
            åŒ…å«ä¼ç¬”åˆ†æä¿¡æ¯çš„å­—å…¸
        """
        overdue = []  # è¿‡æœŸä¼ç¬”
        due_soon = []  # å³å°†åˆ°æœŸçš„ä¼ç¬”
        active = []  # æ‰€æœ‰æ´»è·ƒä¼ç¬”
        
        # ä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–ä¼ç¬”
        if hasattr(state.memory_context, 'structured_foreshadowing') and state.memory_context.structured_foreshadowing:
            for f in state.memory_context.structured_foreshadowing:
                if f.status in [ForeshadowingStatus.PLANTED, ForeshadowingStatus.ADVANCED]:
                    active.append(f)
                    
                    if f.expected_resolve_chapter:
                        if f.expected_resolve_chapter < current_chapter:
                            overdue.append(f)
                        elif f.expected_resolve_chapter <= current_chapter + 3:
                            due_soon.append(f)
            
            # æ„å»ºä¼ç¬”å­—ç¬¦ä¸²
            threads_parts = []
            for f in active:
                urgency = ""
                if f in overdue:
                    urgency = "ã€è¿‡æœŸï¼ã€‘"
                elif f in due_soon:
                    urgency = f"ã€ç¬¬{f.expected_resolve_chapter}ç« åˆ°æœŸã€‘"
                
                importance_star = "â˜…" * min(f.importance, 5)
                threads_parts.append(
                    f"- {urgency}{f.content}ï¼ˆé‡è¦æ€§:{importance_star}ï¼ŒåŸ‹äºç¬¬{f.created_at_chapter}ç« ï¼‰"
                )
            
            threads_str = "\n".join(threads_parts) if threads_parts else "æ— "
        else:
            # å›é€€åˆ°ä¼ ç»Ÿä¼ç¬”åˆ—è¡¨
            traditional = getattr(state.memory_context, "global_foreshadowing", []) or []
            threads_str = "\n".join([f"- {t}" for t in traditional]) if traditional else "æ— "
        
        # æ„å»ºç´§æ€¥æ€§æç¤º
        urgency_parts = []
        if overdue:
            urgency_parts.append(
                f"âš ï¸ ä¸¥é‡è­¦å‘Šï¼šæœ‰ {len(overdue)} ä¸ªä¼ç¬”å·²è¶…è¿‡é¢„æœŸå›æ”¶æ—¶é—´ï¼Œå¿…é¡»å°½å¿«å¤„ç†ï¼"
            )
            for f in overdue[:3]:
                delay = current_chapter - f.expected_resolve_chapter
                urgency_parts.append(
                    f"  - \"{f.content[:30]}...\"ï¼ˆå·²å»¶è¿Ÿ {delay} ç« ï¼ŒåŸå®šç¬¬{f.expected_resolve_chapter}ç« å›æ”¶ï¼‰"
                )
        
        if due_soon and not overdue:
            urgency_parts.append(
                f"ğŸ“Œ æé†’ï¼šæœ‰ {len(due_soon)} ä¸ªä¼ç¬”å³å°†åˆ°æœŸï¼Œè¯·è€ƒè™‘æ¨è¿›æˆ–å›æ”¶ã€‚"
            )
            for f in due_soon[:2]:
                chapters_left = f.expected_resolve_chapter - current_chapter
                urgency_parts.append(
                    f"  - \"{f.content[:30]}...\"ï¼ˆè¿˜å‰© {chapters_left} ç« ï¼‰"
                )
        
        if not urgency_parts:
            urgency_parts.append("å½“å‰æ— ç´§æ€¥éœ€è¦å¤„ç†çš„ä¼ç¬”ã€‚å¯æ ¹æ®å‰§æƒ…éœ€è¦è‡ªç”±æ¨è¿›æˆ–åŸ‹è®¾æ–°ä¼ç¬”ã€‚")
        
        return {
            "threads_str": threads_str,
            "urgency_prompt": "\n".join(urgency_parts),
            "overdue": overdue,
            "due_soon": due_soon,
            "active": active
        }
    
    def _build_overdue_instruction(self, overdue_foreshadowings: List[ForeshadowingSchema]) -> str:
        """
        æ„å»ºè¿‡æœŸä¼ç¬”çš„å¼ºåˆ¶å›æ”¶æŒ‡ä»¤
        
        Args:
            overdue_foreshadowings: è¿‡æœŸä¼ç¬”åˆ—è¡¨
            
        Returns:
            å¼ºåˆ¶å›æ”¶æŒ‡ä»¤æ–‡æœ¬
        """
        lines = ["ã€è¿‡æœŸä¼ç¬”å¼ºåˆ¶å¤„ç†æŒ‡ä»¤ã€‘"]
        lines.append("ä»¥ä¸‹ä¼ç¬”å·²è¶…è¿‡é¢„æœŸå›æ”¶æ—¶é—´ï¼Œæœ¬ç« å¿…é¡»è¿›è¡Œå¤„ç†ï¼ˆæ¨è¿›æˆ–å›æ”¶ï¼‰ï¼š")
        
        for i, f in enumerate(overdue_foreshadowings[:3], 1):
            lines.append(f"{i}. {f.content}")
            if f.resolve_condition:
                lines.append(f"   å›æ”¶æ¡ä»¶ï¼š{f.resolve_condition}")
            if f.resolve_strategy:
                lines.append(f"   å»ºè®®ç­–ç•¥ï¼š{f.resolve_strategy}")
        
        lines.append("\nè¯·åœ¨å†™ä½œæ—¶ç¡®ä¿å¯¹ä¸Šè¿°ä¼ç¬”æœ‰æ˜ç¡®çš„æ¨è¿›æˆ–å›æ”¶ã€‚")
        
        return "\n".join(lines)
